{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "943dcd57-1f27-4ffc-9cb4-a6f6985c7842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "import time\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac8838d5-44a9-4574-b6cf-13903472abaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/jdli/\")\n",
    "\n",
    "# from transformer import TransformerReg,generate_square_subsequent_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6568350-894a-4352-b676-d46557c48084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import GaiaXPlabel_forcast\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_dir = \"/data/jdli/gaia/\"\n",
    "tr_file = \"ap17_wise_xpcont_cut.npy\"\n",
    "\n",
    "device = torch.device('cuda:1')\n",
    "TOTAL_NUM = 1000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "gdata  = GaiaXPlabel_forcast(data_dir+tr_file, total_num=TOTAL_NUM, part_train=True, device=device)\n",
    "\n",
    "val_size = int(0.1*len(gdata))\n",
    "A_size = int(0.5*(len(gdata)-val_size))\n",
    "B_size = len(gdata) - A_size - val_size\n",
    "\n",
    "A_dataset, B_dataset, val_dataset = torch.utils.data.random_split(gdata, [A_size, B_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "print(len(A_dataset), len(B_dataset), len(val_dataset))\n",
    "\n",
    "A_loader = DataLoader(A_dataset, batch_size=BATCH_SIZE)\n",
    "B_loader = DataLoader(B_dataset, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b2e3d664-a82c-4110-9bc1-23847eaf7fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerReg(\n",
    "    dim_val=64, input_size=1,\n",
    "    batch_first=True, \n",
    "    enc_seq_len=30, dec_seq_len=4,\n",
    "    out_seq_len=4, \n",
    "    n_decoder_layers=2,\n",
    "    n_encoder_layers=2, \n",
    "    n_heads=4,\n",
    "    max_seq_len=30,\n",
    ").to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d1597246-0ee4-4a2d-be27-8d508a1e09ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 1])\n",
      "torch.Size([5, 1, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 3, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4758],\n",
       "         [ 0.1749],\n",
       "         [ 0.6493],\n",
       "         [ 0.1224]],\n",
       "\n",
       "        [[-0.0355],\n",
       "         [ 0.1663],\n",
       "         [ 0.0221],\n",
       "         [ 0.2551]],\n",
       "\n",
       "        [[ 0.0329],\n",
       "         [ 0.5259],\n",
       "         [ 0.5337],\n",
       "         [ 0.3079]],\n",
       "\n",
       "        [[-0.0866],\n",
       "         [ 0.2090],\n",
       "         [ 0.2769],\n",
       "         [ 0.4881]],\n",
       "\n",
       "        [[ 0.9540],\n",
       "         [ 0.4800],\n",
       "         [ 0.6724],\n",
       "         [ 0.2874]]], device='cuda:1', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def infer(model: nn.Module, src: torch.Tensor, forecast_window:int,\n",
    "          device,) -> torch.Tensor:\n",
    "    \n",
    "    target_seq_dim = 1\n",
    "    tgt = src[:,-1,0].view(-1, 1, 1) # [bs, 1, 1]\n",
    "    print(tgt.size())\n",
    "    # Iteratively concatenate tgt with the first element in the prediction\n",
    "    for _ in range(forecast_window-1):\n",
    "        \n",
    "        dim_a = tgt.shape[1] #1,2,3,.. n\n",
    "        dim_b = src.shape[1] #30\n",
    "        \n",
    "        src_mask = generate_square_subsequent_mask(dim1=dim_a, dim2=dim_b).to(device)\n",
    "        tgt_mask = generate_square_subsequent_mask(dim1=dim_a, dim2=dim_a).to(device)\n",
    "        \n",
    "        prediction = model(src, tgt, src_mask, tgt_mask)\n",
    "\n",
    "        # Obtain the predicted value at t+1 where t is the last step \n",
    "        # represented in tgt\n",
    "        last_predicted_value = prediction[:,-1,:].view(-1,1,1) #[bs, 1]\n",
    "        \n",
    "        # Reshape from [batch_size, 1] --> [1, batch_size, 1]\n",
    "        # last_predicted_value = last_predicted_value\n",
    "        # print(last_predicted_value.detach(), last_predicted_value.size())\n",
    "        print(tgt.size())\n",
    "        # Detach the predicted element from the graph and concatenate with \n",
    "        # tgt in dimension 1 or 0\n",
    "        tgt = torch.cat((tgt, last_predicted_value), dim=target_seq_dim)\n",
    "    \n",
    "    src_mask = generate_square_subsequent_mask(dim1=4, dim2=30).to(device)\n",
    "    tgt_mask = generate_square_subsequent_mask(dim1=4, dim2=4).to(device)\n",
    "    \n",
    "    # Make final prediction\n",
    "    return model(src, tgt, src_mask, tgt_mask)\n",
    "\n",
    "src = torch.rand(5, 30, 1).to(device)\n",
    "infer(model=model, src=src, forecast_window=4, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9f14caf6-da8f-487c-9d94-1808e12b774f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 tr loss:0.9180 time:1.35 s\n",
      "Epoch #1 tr loss:0.9523 time:1.35 s\n",
      "Epoch #2 tr loss:0.9661 time:1.34 s\n",
      "Epoch #3 tr loss:0.9332 time:1.34 s\n",
      "Epoch #4 tr loss:0.9639 time:1.34 s\n",
      "Epoch #5 tr loss:0.9463 time:1.34 s\n",
      "Epoch #6 tr loss:0.9303 time:1.34 s\n",
      "Epoch #7 tr loss:0.9430 time:1.36 s\n",
      "Epoch #8 tr loss:0.9249 time:1.37 s\n",
      "Epoch #9 tr loss:0.9350 time:1.38 s\n",
      "Epoch #10 tr loss:0.9422 time:1.39 s\n",
      "Epoch #11 tr loss:0.9454 time:1.37 s\n",
      "Epoch #12 tr loss:0.9725 time:1.34 s\n",
      "Epoch #13 tr loss:0.9236 time:1.36 s\n",
      "Epoch #14 tr loss:0.9636 time:1.36 s\n",
      "Epoch #15 tr loss:0.9746 time:1.34 s\n",
      "Epoch #16 tr loss:0.9183 time:1.34 s\n",
      "Epoch #17 tr loss:0.9343 time:1.36 s\n",
      "Epoch #18 tr loss:0.9871 time:1.35 s\n",
      "Epoch #19 tr loss:0.9474 time:1.36 s\n",
      "Epoch #20 tr loss:0.9194 time:1.34 s\n"
     ]
    }
   ],
   "source": [
    "enc_seq_len = 30\n",
    "out_seq_len = 4\n",
    "\n",
    "src_mask = generate_square_subsequent_mask(\n",
    "dim1=4, dim2=30\n",
    ").to(device)\n",
    "\n",
    "tgt_mask = generate_square_subsequent_mask( \n",
    "    dim1=4, dim2=4\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "def train_epoch(tr_loader):\n",
    "    # model.train()\n",
    "    model.train()\n",
    "    # model_mohaom.train()\n",
    "    total_loss = 0.\n",
    "    start = time.time()\n",
    "    for batch, data in enumerate(tr_loader):\n",
    "\n",
    "        output = model(data['x'].view(-1,enc_seq_len,1), \n",
    "        data['y'].view(-1,out_seq_len,1), src_mask, tgt_mask)\n",
    "        loss = cost(output, data['y'].view(-1,out_seq_len,1))\n",
    "        loss_value = loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        total_loss+=loss_value\n",
    "        del data, output\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"Epoch #%d tr loss:%.4f time:%.2f s\"%(epoch, total_loss/(batch+1), (end-start)))\n",
    "\n",
    "\n",
    "def eval(val_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_val_loss = 0\n",
    "        for bs, data in enumerate(val_loader):\n",
    "            # output = model(data['x'], data['tgt'])\n",
    "            output = infer(model=model, src=data['x'].view(-1,enc_seq_len,1), forecast_window=4, device=device)\n",
    "\n",
    "            loss = cost(output, data['y'].view(-1,out_seq_len,1))\n",
    "            total_val_loss+=loss.item()\n",
    "\n",
    "    print(\"val loss:%.4f\"%(total_val_loss/(bs+1)))\n",
    "\n",
    "    # if epoch%5==0:\n",
    "    #     eval(val_loader)\n",
    "for epoch in range(num_epochs+1):\n",
    "    train_epoch(A_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ad2f1352-103e-42a3-82c7-9a065712f0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 1])\n",
      "torch.Size([1, 1, 1])\n",
      "torch.Size([1, 2, 1])\n",
      "torch.Size([1, 3, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2787],\n",
       "         [0.4630],\n",
       "         [0.4972],\n",
       "         [0.6162]]], device='cuda:1', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src = A_dataset[0]['x'].view(-1,enc_seq_len,1)\n",
    "\n",
    "infer(model=model, src=src, forecast_window=4, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "daf347b5-6b22-4e49-a565-82b3ab303bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6115,  1.1501, -0.5797,  0.0771], device='cuda:1')"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_dataset[0]['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b73c3792-a63b-4b83-8e8f-316b714e2960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5737],\n",
       "         [0.0361],\n",
       "         [0.3946],\n",
       "         [0.8979]]], device='cuda:1', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(src, A_dataset[0]['y'].view(-1,4,1),\n",
    "      src_mask, tgt_mask\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "f34cd0ce-5ad9-4dba-b984-cdfc4fef5e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3979],\n",
       "         [-0.1271],\n",
       "         [ 0.4941],\n",
       "         [ 0.5222]]], device='cuda:1', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(src, A_dataset[0]['y'].view(-1,4,1),\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c25ce4b0-4112-4d11-b31c-70a620c71171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.6115],\n",
       "         [ 1.1501],\n",
       "         [-0.5797],\n",
       "         [ 0.0771]]], device='cuda:1')"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " A_dataset[0]['y'].view(-1,4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9dad00d9-f079-4c61-a279-3a78ca0318cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 3000\n"
     ]
    }
   ],
   "source": [
    "from data import APerr\n",
    "\n",
    "data_dir = \"/data/jdli/sdss/\"\n",
    "tr_file = \"hogg19_spec_tr.npy\"\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "TOTAL_NUM = 6000\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "aspcap  = APerr(data_dir+tr_file, total_num=TOTAL_NUM,\n",
    "part_train=True, device=device)\n",
    "\n",
    "# aspcap_val = ASPCAP(data_dir+val_file, device=device)\n",
    "train_size = int(0.5*len(aspcap))\n",
    "val_size = len(aspcap) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(aspcap, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "print(len(train_dataset), len(val_dataset))\n",
    "\n",
    "# tr_loader  = DataLoader(train_dataset, batch_size=BATCH_SIZE, )\n",
    "# val_loader = DataLoader(val_dataset,  batch_size=BATCH_SIZE, )\n",
    "tr_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8e02f0b9-e719-4215-9443-3bc624674c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_val = 64 # This can be any value divisible by n_heads. 512 is used in the original transformer paper.\n",
    "n_heads = 4 # The number of attention heads (aka parallel attention layers). dim_val must be divisible by this number\n",
    "n_decoder_layers = 2 # Number of times the decoder layer is stacked in the decoder\n",
    "n_encoder_layers = 2 # Number of times the encoder layer is stacked in the encoder\n",
    "input_size = 1 # The number of input variables. 1 if univariate forecasting.\n",
    "enc_seq_len = 8575 # length of input given to encoder. Can have any integer value.\n",
    "dec_seq_len = 2 # length of input given to decoder. Can have any integer value.\n",
    "output_sequence_length = 2 # Length of the target sequence, i.e. how many time steps should your forecast\n",
    "max_seq_len = 8575 # What's the longest sequence the model will encounter? Used to make the positional encoder\n",
    "model = TransformerReg(dim_val=dim_val, input_size=input_size, \n",
    "                    batch_first=True, dec_seq_len=dec_seq_len, \n",
    "                    out_seq_len=output_sequence_length, n_decoder_layers=n_decoder_layers,\n",
    "                    n_encoder_layers=n_encoder_layers, n_heads=n_heads,\n",
    "                    max_seq_len=max_seq_len,\n",
    "                    ).to(device)\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83fb21e-1985-4750-82ea-f40af7a8b2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = 0.\n",
    "num_epochs = 20\n",
    "# num_batches = train_size//BATCH_SIZE\n",
    "itr = 1\n",
    "num_iters  = 50\n",
    "\n",
    "src_mask = generate_square_subsequent_mask(\n",
    "dim1=2, dim2=8575\n",
    ").to(device)\n",
    "\n",
    "tgt_mask = generate_square_subsequent_mask(\n",
    "    dim1=2, dim2=2\n",
    ").to(device)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    for batch, (x, y) in enumerate(tr_loader):\n",
    "        start = time.time()\n",
    "\n",
    "        output = model(x, y, src_mask, tgt_mask)\n",
    "        loss = cost(output, y)\n",
    "        loss_value = loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        del x, y, output\n",
    "\n",
    "    print(f\"Epoch #%d tr loss:%.4f time:%.2f s\"%(epoch, total_loss, (end-start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f24c0700-c8af-4229-949d-6357e8a932e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        padding = 1 if torch.__version__>='1.5.0' else 2\n",
    "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model, \n",
    "                                    kernel_size=3, padding=padding, padding_mode='circular')\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight,mode='fan_in',nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1,2)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        # x = x + self.pe[:x.size(0)]\n",
    "        x = x.permute(1,0,2)\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x).permute(1,0,2)\n",
    "\n",
    "\n",
    "class DataEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model, emb_dim=128, dropout=0.1):\n",
    "        super(DataEmbedding, self).__init__()\n",
    "\n",
    "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
    "        self.position_embedding = PositionalEncoding(d_model=d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.position_embedding(x)+self.value_embedding(x)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "    \n",
    "\n",
    "class Spec2label(nn.Module):\n",
    "    def __init__(self, n_encoder_inputs, n_outputs, channels=128,\n",
    "        n_heads=8, n_layers=8, dropout=0.2, attn=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # self.save_hyperparameters()\n",
    "        self.n_encoder_inputs = n_encoder_inputs\n",
    "        self.channels = channels\n",
    "        self.n_outputs = n_outputs\n",
    "        self.dropout = dropout\n",
    "        self.attn = attn\n",
    "        self.enc_embedding = DataEmbedding(1, d_model=channels, dropout=dropout)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=channels,\n",
    "            nhead=n_heads,\n",
    "            dropout=self.dropout,\n",
    "            dim_feedforward=4*channels, batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.input_projection = nn.Linear(n_encoder_inputs, channels)\n",
    "\n",
    "        self.fc1 = nn.Linear(channels*n_encoder_inputs, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 128)\n",
    "        self.fc3 = nn.Linear(128, n_outputs)\n",
    "        self.do = nn.Dropout(p=self.dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        src = self.enc_embedding(x.view(-1, self.n_encoder_inputs, 1)) # (bs, n_enc, dim)\n",
    "        src = self.encoder(src) # (bs, n_enc, dim)\n",
    "        \n",
    "        if self.attn:\n",
    "            attention_maps = []\n",
    "            for l in self.encoder.layers:\n",
    "                attention_maps.append(l.self_attn(src, src, src)[1])\n",
    "        # src = src.view(-1, self.channels*self.n_encoder_inputs) # (bs, 512*113)\n",
    "        src = torch.flatten(src, start_dim=1) #(bs, 512*113)\n",
    "        src = F.relu(self.fc1(src)) # (bs, 1024)\n",
    "        src = F.relu(self.fc2(src)) # (bs, 128)\n",
    "        src = F.relu(self.fc3(src)) # (bs, 1)\n",
    "        tgt = self.do(src)\n",
    "\n",
    "        if self.attn:\n",
    "            return tgt, attention_maps\n",
    "        else:\n",
    "            return tgt\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d65a9c26-f101-4b29-8a35-8918b30af3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transpec.transGaia.data import GaiaXP_allcoefs_5label_cont_norm\n",
    "\n",
    "data_dir = \"/data/jdli/gaia/\"\n",
    "tr_file = \"ap17_wise_xpcont_cut.npy\"\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "\n",
    "gdata  = GaiaXP_allcoefs_5label_cont_norm(\n",
    "        data_dir+tr_file, \n",
    "        part_train=True, \n",
    "        device=device\n",
    "    )\n",
    "\n",
    "gloader = DataLoader(gdata, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c0405e2a-ea6a-42e0-835d-389d2427c55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del Spec2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ee98e4f-99fe-4732-9001-8a4f1d0510f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_LEN = 55*2+3\n",
    "\n",
    "model = Spec2label(\n",
    "    n_encoder_inputs=INPUT_LEN,\n",
    "    n_outputs=1, channels=128, n_heads=8, n_layers=8, attn=True\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # out = model(gdata[0]['x'].view(1,113))\n",
    "    out = model(torch.rand(8 ,113, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d57746a6-1f99-4a81-a16d-bd3cc820cd76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3964ded6-9f2b-4bae-9151-51e1bbf7e665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0057, 0.0091, 0.0076,  ..., 0.0078, 0.0105, 0.0098],\n",
       "        [0.0080, 0.0107, 0.0105,  ..., 0.0090, 0.0083, 0.0099],\n",
       "        [0.0080, 0.0104, 0.0104,  ..., 0.0092, 0.0098, 0.0075],\n",
       "        ...,\n",
       "        [0.0067, 0.0104, 0.0118,  ..., 0.0091, 0.0069, 0.0087],\n",
       "        [0.0085, 0.0090, 0.0056,  ..., 0.0076, 0.0099, 0.0105],\n",
       "        [0.0111, 0.0067, 0.0103,  ..., 0.0087, 0.0083, 0.0098]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att = out[1][0][0]\n",
    "\n",
    "att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "482592c0-ac76-4c0f-991e-c442566fe606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([113, 113])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4db909db-c864-43e9-b07c-4866a25e4ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1, 1024, 512)\n",
    "multihead_attn = nn.MultiheadAttention(512, 8, batch_first=True)\n",
    "attn_output, attn_output_weights = multihead_attn(x, x, x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "825d6e36-4e4a-4e57-8453-4af6713a2811",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transpec.transGaia.transgaia import Spec2label\n",
    "\n",
    "\n",
    "model = Spec2label(\n",
    "        n_encoder_inputs=INPUT_LEN,\n",
    "        n_outputs=1, channels=512, n_heads=8, n_layers=8,\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "990666e3-d148-429f-92a2-f24c815a0136",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(gloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "a22ede39-1c65-485b-9a5c-2614769e402a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    # out = model(gdata[0]['x'].view(1,113))\n",
    "    out = model(data['x'].view(-1,113))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "c419e398-1611-4d1b-a1f3-0bf6123ccf4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "742c5b96-02a7-48f9-a34c-367c4234599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Conv1d(16, 33, 3, stride=2)\n",
    "input = torch.randn(20, 16, 50)\n",
    "output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7e2ac0fe-e503-4ab5-b100-2161ef83ead1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        padding = 1 if torch.__version__>='1.5.0' else 2\n",
    "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model, \n",
    "                                    kernel_size=3, padding=padding, padding_mode='circular')\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight,mode='fan_in',nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1,2)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class DataEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model, dropout=0.1):\n",
    "        super(DataEmbedding, self).__init__()\n",
    "\n",
    "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
    "        self.position_embedding = PositionalEncoding(d_model=d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.position_embedding(x)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "embd = DataEmbedding(c_in=1, d_model=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "827e8b5e-d7a9-41b5-bdc2-443c61295d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 113, 128])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(8, 113, 1)\n",
    "\n",
    "embd(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a0e4bdfa-68b5-4a5c-ab8d-ecb263086eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 113, 128])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model:int, dropout:float=0.1, max_len:int=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0)/d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        # x = x + self.pe[:x.size(0)]\n",
    "        x = x.permute(1,0,2)\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x).permute(1,0,2)\n",
    "    \n",
    "pos_enc = PositionalEncoding(d_model=128)\n",
    "\n",
    "pos_enc(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c3f99569-d6fc-4e7d-bb2b-67783bb12d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Spec2label(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_encoder_inputs,\n",
    "        n_outputs,\n",
    "        channels=64,\n",
    "        n_heads=8,\n",
    "        n_layers=8,\n",
    "        dropout=0.2,\n",
    "        attn=False,\n",
    "        lr=1e-4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # self.save_hyperparameters()\n",
    "        self.channels = channels\n",
    "        self.n_outputs = n_outputs\n",
    "        self.lr = lr\n",
    "        self.dropout = dropout\n",
    "        self.input_pos_embedding  = torch.nn.Embedding(128, embedding_dim=channels)\n",
    "        self.attn = attn\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=channels, nhead=n_heads,\n",
    "            dropout=self.dropout, dim_feedforward=4*channels, batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.input_projection = nn.Linear(1, channels)\n",
    "        # self.output_projection = Linear(n_decoder_inputs, channels)\n",
    "        # self.linear = Linear(channels, 2)\n",
    "        self.fc1 = nn.Linear(channels*n_encoder_inputs, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 64)\n",
    "        self.fc3 = nn.Linear(64, n_outputs)\n",
    "        self.do = nn.Dropout(p=self.dropout)\n",
    "\n",
    "    def encode_src(self, src):\n",
    "        src_start = self.input_projection(src)\n",
    "        in_sequence_len, batch_size = src_start.size(1), src_start.size(0)\n",
    "        pos_encoder = (\n",
    "            torch.arange(0, in_sequence_len, device=src.device)\n",
    "            .unsqueeze(0).repeat(batch_size, 1)\n",
    "        )\n",
    "        pos_encoder = self.input_pos_embedding(pos_encoder)\n",
    "        print(pos_encoder.shape)\n",
    "        print(src_start.shape)\n",
    "        src = src_start + pos_encoder\n",
    "\n",
    "        if self.attn:\n",
    "            attention_maps = []\n",
    "            for l in self.encoder.layers:\n",
    "                attention_maps.append(l.self_attn(src, src, src)[1])\n",
    "\n",
    "            src = self.encoder(src)\n",
    "            return src, attention_maps\n",
    "        else:\n",
    "            src = self.encoder(src)\n",
    "            return src\n",
    "\n",
    "    def forward(self, x):\n",
    "        src = x\n",
    "        if self.attn:\n",
    "            src, attention_maps = self.encode_src(src) # (1, bs, 512)\n",
    "        else:\n",
    "            src = self.encode_src(src)\n",
    "    \n",
    "        src = torch.flatten(src, start_dim=1)\n",
    "        src = self.do(src)\n",
    "        src = F.relu(self.fc1(src))\n",
    "        src = F.relu(self.fc2(src))\n",
    "        tgt = F.relu(self.fc3(src)) # (bs, 1)\n",
    "        \n",
    "        if self.attn:\n",
    "            return tgt, attention_maps\n",
    "        else:\n",
    "            return tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b5855f8e-b77c-4985-8f66-3ef5de5c59fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(4, 113, 1)\n",
    "\n",
    "model = Spec2label(\n",
    "    n_encoder_inputs=55*2+3,\n",
    "    n_outputs=1, channels=128, n_heads=8, n_layers=8,\n",
    "    attn=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "12ebe54c-683f-430b-85f2-4edbac7df98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 113, 128])\n",
      "torch.Size([4, 113, 128])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (4x14464 and 128x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [149]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/data/jdli/anaconda3/envs/gaia/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [147]\u001b[0m, in \u001b[0;36mSpec2label.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     63\u001b[0m     src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_src(src)\n\u001b[1;32m     65\u001b[0m src \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(src, start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 67\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (bs, 64)\u001b[39;00m\n\u001b[1;32m     68\u001b[0m src \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(src)\n\u001b[1;32m     69\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo(src)\n",
      "File \u001b[0;32m/data/jdli/anaconda3/envs/gaia/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/jdli/anaconda3/envs/gaia/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (4x14464 and 128x64)"
     ]
    }
   ],
   "source": [
    "model(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b547c3-63cd-4135-9e35-f6df7616af98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
