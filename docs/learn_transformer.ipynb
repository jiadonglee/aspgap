{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "943dcd57-1f27-4ffc-9cb4-a6f6985c7842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "import time\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ac8838d5-44a9-4574-b6cf-13903472abaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/jdli/TransSpectra/\")\n",
    "\n",
    "from transformer import TransformerReg,generate_square_subsequent_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a6568350-894a-4352-b676-d46557c48084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450 450 100\n"
     ]
    }
   ],
   "source": [
    "from data import GaiaXPlabel_forcast\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_dir = \"/data/jdli/gaia/\"\n",
    "tr_file = \"ap17_wise_xpcont_cut.npy\"\n",
    "\n",
    "device = torch.device('cuda:1')\n",
    "TOTAL_NUM = 1000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "gdata  = GaiaXPlabel_forcast(data_dir+tr_file, total_num=TOTAL_NUM, part_train=True, device=device)\n",
    "\n",
    "val_size = int(0.1*len(gdata))\n",
    "A_size = int(0.5*(len(gdata)-val_size))\n",
    "B_size = len(gdata) - A_size - val_size\n",
    "\n",
    "A_dataset, B_dataset, val_dataset = torch.utils.data.random_split(gdata, [A_size, B_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "print(len(A_dataset), len(B_dataset), len(val_dataset))\n",
    "\n",
    "A_loader = DataLoader(A_dataset, batch_size=BATCH_SIZE)\n",
    "B_loader = DataLoader(B_dataset, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b2e3d664-a82c-4110-9bc1-23847eaf7fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerReg(\n",
    "    dim_val=64, input_size=1,\n",
    "    batch_first=True, \n",
    "    enc_seq_len=30, dec_seq_len=4,\n",
    "    out_seq_len=4, \n",
    "    n_decoder_layers=2,\n",
    "    n_encoder_layers=2, \n",
    "    n_heads=4,\n",
    "    max_seq_len=30,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# dim_val = 64 # This can be any value divisible by n_heads. 512 is used in the original transformer paper.\n",
    "# n_heads = 4 # The number of attention heads (aka parallel attention layers). dim_val must be divisible by this number\n",
    "# n_decoder_layers = 2 # Number of times the decoder layer is stacked in the decoder\n",
    "# n_encoder_layers = 2 # Number of times the encoder layer is stacked in the encoder\n",
    "# input_size = 1 # The number of input variables. 1 if univariate forecasting.\n",
    "# enc_seq_len = 8575 # length of input given to encoder. Can have any integer value.\n",
    "# dec_seq_len = 2 # length of input given to decoder. Can have any integer value.\n",
    "# output_sequence_length = 2 # Length of the target sequence, i.e. how many time steps should your forecast\n",
    "# max_seq_len = 8575 # What's the longest sequence the model will encounter? Used to make the positional encoder\n",
    "# model = TransformerReg(dim_val=dim_val, input_size=input_size, \n",
    "#                     batch_first=True, dec_seq_len=dec_seq_len, \n",
    "#                     out_seq_len=output_sequence_length, n_decoder_layers=n_decoder_layers,\n",
    "#                     n_encoder_layers=n_encoder_layers, n_heads=n_heads,\n",
    "#                     max_seq_len=max_seq_len,\n",
    "#                     ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d1597246-0ee4-4a2d-be27-8d508a1e09ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 1])\n",
      "torch.Size([5, 1, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 3, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4758],\n",
       "         [ 0.1749],\n",
       "         [ 0.6493],\n",
       "         [ 0.1224]],\n",
       "\n",
       "        [[-0.0355],\n",
       "         [ 0.1663],\n",
       "         [ 0.0221],\n",
       "         [ 0.2551]],\n",
       "\n",
       "        [[ 0.0329],\n",
       "         [ 0.5259],\n",
       "         [ 0.5337],\n",
       "         [ 0.3079]],\n",
       "\n",
       "        [[-0.0866],\n",
       "         [ 0.2090],\n",
       "         [ 0.2769],\n",
       "         [ 0.4881]],\n",
       "\n",
       "        [[ 0.9540],\n",
       "         [ 0.4800],\n",
       "         [ 0.6724],\n",
       "         [ 0.2874]]], device='cuda:1', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def infer(model: nn.Module, src: torch.Tensor, forecast_window:int,\n",
    "          device,) -> torch.Tensor:\n",
    "    \n",
    "    target_seq_dim = 1\n",
    "    tgt = src[:,-1,0].view(-1, 1, 1) # [bs, 1, 1]\n",
    "    print(tgt.size())\n",
    "    # Iteratively concatenate tgt with the first element in the prediction\n",
    "    for _ in range(forecast_window-1):\n",
    "        \n",
    "        dim_a = tgt.shape[1] #1,2,3,.. n\n",
    "        dim_b = src.shape[1] #30\n",
    "        \n",
    "        src_mask = generate_square_subsequent_mask(dim1=dim_a, dim2=dim_b).to(device)\n",
    "        tgt_mask = generate_square_subsequent_mask(dim1=dim_a, dim2=dim_a).to(device)\n",
    "        \n",
    "        prediction = model(src, tgt, src_mask, tgt_mask)\n",
    "\n",
    "        # Obtain the predicted value at t+1 where t is the last step \n",
    "        # represented in tgt\n",
    "        last_predicted_value = prediction[:,-1,:].view(-1,1,1) #[bs, 1]\n",
    "        \n",
    "        # Reshape from [batch_size, 1] --> [1, batch_size, 1]\n",
    "        # last_predicted_value = last_predicted_value\n",
    "        # print(last_predicted_value.detach(), last_predicted_value.size())\n",
    "        print(tgt.size())\n",
    "        # Detach the predicted element from the graph and concatenate with \n",
    "        # tgt in dimension 1 or 0\n",
    "        tgt = torch.cat((tgt, last_predicted_value), dim=target_seq_dim)\n",
    "    \n",
    "    src_mask = generate_square_subsequent_mask(dim1=4, dim2=30).to(device)\n",
    "    tgt_mask = generate_square_subsequent_mask(dim1=4, dim2=4).to(device)\n",
    "    \n",
    "    # Make final prediction\n",
    "    return model(src, tgt, src_mask, tgt_mask)\n",
    "\n",
    "src = torch.rand(5, 30, 1).to(device)\n",
    "infer(model=model, src=src, forecast_window=4, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9f14caf6-da8f-487c-9d94-1808e12b774f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 tr loss:0.9180 time:1.35 s\n",
      "Epoch #1 tr loss:0.9523 time:1.35 s\n",
      "Epoch #2 tr loss:0.9661 time:1.34 s\n",
      "Epoch #3 tr loss:0.9332 time:1.34 s\n",
      "Epoch #4 tr loss:0.9639 time:1.34 s\n",
      "Epoch #5 tr loss:0.9463 time:1.34 s\n",
      "Epoch #6 tr loss:0.9303 time:1.34 s\n",
      "Epoch #7 tr loss:0.9430 time:1.36 s\n",
      "Epoch #8 tr loss:0.9249 time:1.37 s\n",
      "Epoch #9 tr loss:0.9350 time:1.38 s\n",
      "Epoch #10 tr loss:0.9422 time:1.39 s\n",
      "Epoch #11 tr loss:0.9454 time:1.37 s\n",
      "Epoch #12 tr loss:0.9725 time:1.34 s\n",
      "Epoch #13 tr loss:0.9236 time:1.36 s\n",
      "Epoch #14 tr loss:0.9636 time:1.36 s\n",
      "Epoch #15 tr loss:0.9746 time:1.34 s\n",
      "Epoch #16 tr loss:0.9183 time:1.34 s\n",
      "Epoch #17 tr loss:0.9343 time:1.36 s\n",
      "Epoch #18 tr loss:0.9871 time:1.35 s\n",
      "Epoch #19 tr loss:0.9474 time:1.36 s\n",
      "Epoch #20 tr loss:0.9194 time:1.34 s\n"
     ]
    }
   ],
   "source": [
    "enc_seq_len = 30\n",
    "out_seq_len = 4\n",
    "\n",
    "src_mask = generate_square_subsequent_mask(\n",
    "dim1=4, dim2=30\n",
    ").to(device)\n",
    "\n",
    "tgt_mask = generate_square_subsequent_mask( \n",
    "    dim1=4, dim2=4\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "def train_epoch(tr_loader):\n",
    "    # model.train()\n",
    "    model.train()\n",
    "    # model_mohaom.train()\n",
    "    total_loss = 0.\n",
    "    start = time.time()\n",
    "    for batch, data in enumerate(tr_loader):\n",
    "\n",
    "        output = model(data['x'].view(-1,enc_seq_len,1), \n",
    "        data['y'].view(-1,out_seq_len,1), src_mask, tgt_mask)\n",
    "        loss = cost(output, data['y'].view(-1,out_seq_len,1))\n",
    "        loss_value = loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        total_loss+=loss_value\n",
    "        del data, output\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"Epoch #%d tr loss:%.4f time:%.2f s\"%(epoch, total_loss/(batch+1), (end-start)))\n",
    "\n",
    "\n",
    "def eval(val_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_val_loss = 0\n",
    "        for bs, data in enumerate(val_loader):\n",
    "            # output = model(data['x'], data['tgt'])\n",
    "            output = infer(model=model, src=data['x'].view(-1,enc_seq_len,1), forecast_window=4, device=device)\n",
    "\n",
    "            loss = cost(output, data['y'].view(-1,out_seq_len,1))\n",
    "            total_val_loss+=loss.item()\n",
    "\n",
    "    print(\"val loss:%.4f\"%(total_val_loss/(bs+1)))\n",
    "\n",
    "    # if epoch%5==0:\n",
    "    #     eval(val_loader)\n",
    "for epoch in range(num_epochs+1):\n",
    "    train_epoch(A_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ad2f1352-103e-42a3-82c7-9a065712f0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 1])\n",
      "torch.Size([1, 1, 1])\n",
      "torch.Size([1, 2, 1])\n",
      "torch.Size([1, 3, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2787],\n",
       "         [0.4630],\n",
       "         [0.4972],\n",
       "         [0.6162]]], device='cuda:1', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src = A_dataset[0]['x'].view(-1,enc_seq_len,1)\n",
    "\n",
    "infer(model=model, src=src, forecast_window=4, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "daf347b5-6b22-4e49-a565-82b3ab303bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6115,  1.1501, -0.5797,  0.0771], device='cuda:1')"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_dataset[0]['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b73c3792-a63b-4b83-8e8f-316b714e2960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5737],\n",
       "         [0.0361],\n",
       "         [0.3946],\n",
       "         [0.8979]]], device='cuda:1', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(src, A_dataset[0]['y'].view(-1,4,1),\n",
    "      src_mask, tgt_mask\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "f34cd0ce-5ad9-4dba-b984-cdfc4fef5e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3979],\n",
       "         [-0.1271],\n",
       "         [ 0.4941],\n",
       "         [ 0.5222]]], device='cuda:1', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(src, A_dataset[0]['y'].view(-1,4,1),\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c25ce4b0-4112-4d11-b31c-70a620c71171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.6115],\n",
       "         [ 1.1501],\n",
       "         [-0.5797],\n",
       "         [ 0.0771]]], device='cuda:1')"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " A_dataset[0]['y'].view(-1,4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9dad00d9-f079-4c61-a279-3a78ca0318cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 3000\n"
     ]
    }
   ],
   "source": [
    "from data import APerr\n",
    "\n",
    "data_dir = \"/data/jdli/sdss/\"\n",
    "tr_file = \"hogg19_spec_tr.npy\"\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "TOTAL_NUM = 6000\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "aspcap  = APerr(data_dir+tr_file, total_num=TOTAL_NUM,\n",
    "part_train=True, device=device)\n",
    "\n",
    "# aspcap_val = ASPCAP(data_dir+val_file, device=device)\n",
    "train_size = int(0.5*len(aspcap))\n",
    "val_size = len(aspcap) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(aspcap, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "print(len(train_dataset), len(val_dataset))\n",
    "\n",
    "# tr_loader  = DataLoader(train_dataset, batch_size=BATCH_SIZE, )\n",
    "# val_loader = DataLoader(val_dataset,  batch_size=BATCH_SIZE, )\n",
    "tr_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8e02f0b9-e719-4215-9443-3bc624674c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_val = 64 # This can be any value divisible by n_heads. 512 is used in the original transformer paper.\n",
    "n_heads = 4 # The number of attention heads (aka parallel attention layers). dim_val must be divisible by this number\n",
    "n_decoder_layers = 2 # Number of times the decoder layer is stacked in the decoder\n",
    "n_encoder_layers = 2 # Number of times the encoder layer is stacked in the encoder\n",
    "input_size = 1 # The number of input variables. 1 if univariate forecasting.\n",
    "enc_seq_len = 8575 # length of input given to encoder. Can have any integer value.\n",
    "dec_seq_len = 2 # length of input given to decoder. Can have any integer value.\n",
    "output_sequence_length = 2 # Length of the target sequence, i.e. how many time steps should your forecast\n",
    "max_seq_len = 8575 # What's the longest sequence the model will encounter? Used to make the positional encoder\n",
    "model = TransformerReg(dim_val=dim_val, input_size=input_size, \n",
    "                    batch_first=True, dec_seq_len=dec_seq_len, \n",
    "                    out_seq_len=output_sequence_length, n_decoder_layers=n_decoder_layers,\n",
    "                    n_encoder_layers=n_encoder_layers, n_heads=n_heads,\n",
    "                    max_seq_len=max_seq_len,\n",
    "                    ).to(device)\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83fb21e-1985-4750-82ea-f40af7a8b2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = 0.\n",
    "num_epochs = 20\n",
    "# num_batches = train_size//BATCH_SIZE\n",
    "itr = 1\n",
    "num_iters  = 50\n",
    "\n",
    "src_mask = generate_square_subsequent_mask(\n",
    "dim1=2, dim2=8575\n",
    ").to(device)\n",
    "\n",
    "tgt_mask = generate_square_subsequent_mask(\n",
    "    dim1=2, dim2=2\n",
    ").to(device)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    for batch, (x, y) in enumerate(tr_loader):\n",
    "        start = time.time()\n",
    "\n",
    "        output = model(x, y, src_mask, tgt_mask)\n",
    "        loss = cost(output, y)\n",
    "        loss_value = loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        del x, y, output\n",
    "\n",
    "    print(f\"Epoch #%d tr loss:%.4f time:%.2f s\"%(epoch, total_loss, (end-start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24c0700-c8af-4229-949d-6357e8a932e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
